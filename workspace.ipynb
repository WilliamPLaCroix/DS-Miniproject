{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp500_files: set = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/snp500'))\n",
    "print(len(snp500_files))\n",
    "nasdaq_files: set = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nasdaq')) - snp500_files\n",
    "print(len(nasdaq_files))\n",
    "nyse_files: set = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nyse')) - snp500_files\n",
    "print(len(nyse_files))\n",
    "nasdaq_nyse: set = nasdaq_files.union(nyse_files)\n",
    "print(len(nasdaq_nyse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/snp500/AMZN.csv')\n",
    "initial_stock_data['Date'] = initial_stock_data['Date'].apply(lambda x: x[-4:])\n",
    "annual_data = initial_stock_data.groupby('Date').mean()\n",
    "annual_data['Annual Percent Change'] = annual_data['Close'].pct_change()\n",
    "annual_data = annual_data.fillna(0)\n",
    "print(annual_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/toy_data/analyst_ratings_processed.csv', index_col=0)\n",
    "news_data['date'] = news_data['date'].apply(lambda x: str(x)[:4])\n",
    "print(news_data.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "def get_sentiment(input_text: str, model=model, tokenizer=tokenizer, device=device):\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits.to(device)\n",
    "\n",
    "    return torch.nn.Softmax(dim=1)(logits)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_by_stock = news_data.groupby('stock')\n",
    "already_processed = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/stock_data'))\n",
    "for stock in tqdm(news_by_stock):\n",
    "    if stock[0] + '.csv' in already_processed:\n",
    "        continue\n",
    "    try:\n",
    "        stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/snp500/' + stock[0] + '.csv')\n",
    "        label = 1\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nasdaq/' + stock[0] + '.csv')\n",
    "            label = 0\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nyse/' + stock[0] + '.csv')\n",
    "                label = 0\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "    stock_data['Date'] = stock_data['Date'].apply(lambda x: x[-4:])\n",
    "    annual_data = stock_data.groupby('Date').mean()\n",
    "    annual_data['Annual Percent Change'] = annual_data['Close'].pct_change()\n",
    "    annual_data = annual_data.fillna(0)\n",
    "\n",
    "    news_by_year = stock[1].groupby('date')\n",
    "    sentiments_frame = pd.DataFrame(columns=['positive', 'negative', 'neutral'])\n",
    "    sentiment_index = [year[0] for year in news_by_year]\n",
    "    for year in news_by_year:\n",
    "        stories = [year[1]['title'].iloc[:5].tolist()][0]\n",
    "\n",
    "        sentiments = pd.DataFrame(columns=['positive', 'negative', 'neutral'], index=[year[0]])\n",
    "        for story in stories:\n",
    "            sentiment = get_sentiment(story)\n",
    "            sentiments = sentiments.append({'positive': sentiment[0], 'negative': sentiment[1], \n",
    "                               'neutral': sentiment[2]}, ignore_index=True)\n",
    "        \n",
    "        mean_sentiments = sentiments.mean()\n",
    "        sentiments_frame = sentiments_frame.append(mean_sentiments, ignore_index=True)\n",
    "    sentiments_frame.index = sentiment_index\n",
    "    combined_data = pd.concat([annual_data, sentiments_frame], axis=1)\n",
    "    combined_data = combined_data.fillna(combined_data.mean())\n",
    "    combined_data['Label'] = label\n",
    "    combined_data = combined_data.reset_index(drop=True)\n",
    "    combined_data.drop(combined_data[combined_data.Volume == 0].index, inplace=True)\n",
    "    # combined_data.drop(columns=['Date'], inplace=True)\n",
    "    # combined_data.to_csv(f'G:/My Drive/UdS/Classes/Data Science/stock_data/{stock[0]}.csv', index=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/stock_data')\n",
    "#processed_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/truncated_data')\n",
    "\n",
    "recent_year: dict = {}\n",
    "for stock in tqdm(prepared_stock_list):\n",
    "    if stock == 'desktop.ini':\n",
    "        continue\n",
    "    # if stock in processed_stock_list:\n",
    "    #     continue\n",
    "    stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/stock_data/' + stock)\n",
    "    try:\n",
    "        recent_year[stock[:-4]] = stock_data.iloc[-1]\n",
    "    except IndexError:\n",
    "        print(stock)\n",
    "        print(stock_data)\n",
    "        print(stock_data.head())\n",
    "        continue\n",
    "    # write to csv, truncating first and last line\n",
    "    #stock_data[1:-1].to_csv('G:/My Drive/UdS/Classes/Data Science/truncated_data/' + stock, index=False)\n",
    "    \n",
    "recent_year_dataframe = pd.DataFrame.from_dict(recent_year, orient='index')\n",
    "\n",
    "# recent_year_dataframe.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/toy_data/stonks.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/truncated_data')\n",
    "#processed_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/truncated_data')\n",
    "\n",
    "columns = [\"Low\",\"Open\",\"Volume\",\"High\",\"Close\",\"Adjusted Close\",\"Annual Percent Change\",\"positive\",\"negative\",\"neutral\",\"Label\"]\n",
    "full_dataset = pd.DataFrame(columns=columns)\n",
    "for stock in tqdm(truncated_stock_list):\n",
    "    if stock == 'desktop.ini':\n",
    "        continue\n",
    "    stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/stock_data/' + stock)\n",
    "    try:\n",
    "        full_dataset = full_dataset.append(stock_data, ignore_index=True)\n",
    "    except IndexError:\n",
    "        print(stock)\n",
    "        print(stock_data)\n",
    "        print(stock_data.head())\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = full_dataset[\"Label\"]\n",
    "X = full_dataset.drop(columns=[\"Label\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 80:20 train to test\n",
    "X_test, X_dev, y_test, y_dev = train_test_split(X_test, y_test, test_size=0.5, random_state=1) # 50:50 test to dev > 80:10:10 train:dev:test\n",
    "\n",
    "# write splits to individual csv files\n",
    "# X_train.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_train.csv', index=False)\n",
    "# X_dev.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_dev.csv', index=False)\n",
    "# X_test.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_test.csv', index=False)\n",
    "# y_train.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_train.csv', index=False)\n",
    "# y_dev.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_dev.csv', index=False)\n",
    "# y_test.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45859, 10)\n",
      "(5733, 10)\n",
      "(5732, 10)\n",
      "(45859, 1)\n",
      "(5733, 1)\n",
      "(5732, 1)\n"
     ]
    }
   ],
   "source": [
    "# reload splits from csv files\n",
    "X_train = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_train.csv')\n",
    "X_dev = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_dev.csv')\n",
    "X_test = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_test.csv')\n",
    "y_train = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_train.csv')\n",
    "y_dev = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_dev.csv')\n",
    "y_test = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_test.csv')\n",
    "\n",
    "# verify data integrity\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_dev.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     54\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 55\u001b[0m         optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     57\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[0;32m     59\u001b[0m torch\u001b[39m.\u001b[39msave(model, \u001b[39m\"\u001b[39m\u001b[39mG:/My Drive/UdS/Classes/Data Science/DS-Miniproject/models/multilayer_model.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:351\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    354\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_X = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "train_y = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "traindata = MyDataset(train_X, train_y)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(10, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 1),\n",
    "        nn.Softmax()\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 20\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    for X, y in trainloader:\n",
    "        X = X\n",
    "        y = y.squeeze(1)\n",
    "        y_pred = model(X).squeeze(1)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(model)\n",
    "\n",
    "torch.save(model, \"G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/models/multilayer_model.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (7): Softmax(dim=None)\n",
      ")\n",
      "Predicted     1   All\n",
      "True                 \n",
      "0          4970  4970\n",
      "1           763   763\n",
      "All        5733  5733\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     accuracy \u001b[39m=\u001b[39m (confusion[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m confusion[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m confusion[\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3458\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3459\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3362\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m     accuracy \u001b[39m=\u001b[39m (confusion[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m confusion[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m confusion[\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     accuracy \u001b[39m=\u001b[39m (confusion[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m confusion[\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maccuracy:\u001b[39m\u001b[39m\"\u001b[39m, accuracy)\n\u001b[0;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3458\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3459\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3460\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n\u001b[0;32m   3366\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "dev_X = torch.tensor(X_dev.values, dtype=torch.float32)\n",
    "dev_y = torch.tensor(y_dev.values, dtype=torch.long)\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "devdata = MyDataset(dev_X, dev_y)\n",
    "devloader = torch.utils.data.DataLoader(devdata, batch_size=64, shuffle=True)\n",
    "\n",
    "model = torch.load(\"G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/models/multilayer_model.pickle\")\n",
    "print(model)\n",
    "model.eval()\n",
    "\n",
    "dev_predictions = pd.DataFrame(columns=['y_pred', 'y_true'])\n",
    "with torch.no_grad():\n",
    "    for X, y in devloader:\n",
    "        X = X\n",
    "        y = y.squeeze(1).tolist()\n",
    "        \n",
    "        y_pred = [int(y_val) for y_val in model(X).squeeze(1).tolist()]\n",
    "        y_frame = pd.DataFrame({'y_pred': y_pred, 'y_true': y})\n",
    "        dev_predictions = dev_predictions.append(y_frame, ignore_index=True)\n",
    "\n",
    "confusion = pd.crosstab(dev_predictions['y_true'], dev_predictions['y_pred'], rownames=['True'], \n",
    "                        colnames=['Predicted'], margins=True)\n",
    "print(confusion)\n",
    "\n",
    "try:\n",
    "    accuracy = (confusion[0][0] + confusion[1][1]) / confusion['All']['All']\n",
    "except KeyError:\n",
    "    accuracy = (confusion[0][0]) / confusion['All']['All']\n",
    "print(\"accuracy:\", accuracy)\n",
    "try:\n",
    "    precision = confusion[1][1] / confusion['All'][1]\n",
    "except KeyError:\n",
    "    precision = 0\n",
    "print(\"precision:\", precision)\n",
    "try:\n",
    "    recall = confusion[1][1] / confusion[1]['All']\n",
    "except KeyError:\n",
    "    recall = 0\n",
    "print(\"recall:\", recall)\n",
    "try:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "except ZeroDivisionError:\n",
    "    f1 = 0\n",
    "print(\"f1:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
