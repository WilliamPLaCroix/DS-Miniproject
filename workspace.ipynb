{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp500_files: set = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/snp500'))\n",
    "print(len(snp500_files))\n",
    "nasdaq_files: set = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nasdaq')) - snp500_files\n",
    "print(len(nasdaq_files))\n",
    "nyse_files: set = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nyse')) - snp500_files\n",
    "print(len(nyse_files))\n",
    "nasdaq_nyse: set = nasdaq_files.union(nyse_files)\n",
    "print(len(nasdaq_nyse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/snp500/AMZN.csv')\n",
    "initial_stock_data['Date'] = initial_stock_data['Date'].apply(lambda x: x[-4:])\n",
    "annual_data = initial_stock_data.groupby('Date').mean()\n",
    "annual_data['Annual Percent Change'] = annual_data['Close'].pct_change()\n",
    "annual_data = annual_data.fillna(0)\n",
    "print(annual_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/toy_data/analyst_ratings_processed.csv', index_col=0)\n",
    "news_data['date'] = news_data['date'].apply(lambda x: str(x)[:4])\n",
    "print(news_data.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "def get_sentiment(input_text: str, model=model, tokenizer=tokenizer, device=device):\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits.to(device)\n",
    "\n",
    "    return torch.nn.Softmax(dim=1)(logits)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_by_stock = news_data.groupby('stock')\n",
    "already_processed = set(os.listdir('G:/My Drive/UdS/Classes/Data Science/stock_data'))\n",
    "for stock in tqdm(news_by_stock):\n",
    "    if stock[0] + '.csv' in already_processed:\n",
    "        continue\n",
    "    try:\n",
    "        stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/snp500/' + stock[0] + '.csv')\n",
    "        label = 1\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nasdaq/' + stock[0] + '.csv')\n",
    "            label = 0\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/nasdaq_snp500_nyse/nyse/' + stock[0] + '.csv')\n",
    "                label = 0\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "    stock_data['Date'] = stock_data['Date'].apply(lambda x: x[-4:])\n",
    "    annual_data = stock_data.groupby('Date').mean()\n",
    "    annual_data['Annual Percent Change'] = annual_data['Close'].pct_change()\n",
    "    annual_data = annual_data.fillna(0)\n",
    "\n",
    "    news_by_year = stock[1].groupby('date')\n",
    "    sentiments_frame = pd.DataFrame(columns=['positive', 'negative', 'neutral'])\n",
    "    sentiment_index = [year[0] for year in news_by_year]\n",
    "    for year in news_by_year:\n",
    "        stories = [year[1]['title'].iloc[:5].tolist()][0]\n",
    "\n",
    "        sentiments = pd.DataFrame(columns=['positive', 'negative', 'neutral'], index=[year[0]])\n",
    "        for story in stories:\n",
    "            sentiment = get_sentiment(story)\n",
    "            sentiments = sentiments.append({'positive': sentiment[0], 'negative': sentiment[1], \n",
    "                               'neutral': sentiment[2]}, ignore_index=True)\n",
    "        \n",
    "        mean_sentiments = sentiments.mean()\n",
    "        sentiments_frame = sentiments_frame.append(mean_sentiments, ignore_index=True)\n",
    "    sentiments_frame.index = sentiment_index\n",
    "    combined_data = pd.concat([annual_data, sentiments_frame], axis=1)\n",
    "    combined_data = combined_data.fillna(combined_data.mean())\n",
    "    combined_data['Label'] = label\n",
    "    combined_data = combined_data.reset_index(drop=True)\n",
    "    combined_data.drop(combined_data[combined_data.Volume == 0].index, inplace=True)\n",
    "    # combined_data.drop(columns=['Date'], inplace=True)\n",
    "    # combined_data.to_csv(f'G:/My Drive/UdS/Classes/Data Science/stock_data/{stock[0]}.csv', index=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/stock_data')\n",
    "#processed_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/truncated_data')\n",
    "\n",
    "recent_year: dict = {}\n",
    "for stock in tqdm(prepared_stock_list):\n",
    "    if stock == 'desktop.ini':\n",
    "        continue\n",
    "    # if stock in processed_stock_list:\n",
    "    #     continue\n",
    "    stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/stock_data/' + stock)\n",
    "    try:\n",
    "        recent_year[stock[:-4]] = stock_data.iloc[-1]\n",
    "    except IndexError:\n",
    "        print(stock)\n",
    "        print(stock_data)\n",
    "        print(stock_data.head())\n",
    "        continue\n",
    "    # write to csv, truncating first and last line\n",
    "    #stock_data[1:-1].to_csv('G:/My Drive/UdS/Classes/Data Science/truncated_data/' + stock, index=False)\n",
    "    \n",
    "recent_year_dataframe = pd.DataFrame.from_dict(recent_year, orient='index')\n",
    "\n",
    "# recent_year_dataframe.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/toy_data/stonks.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/truncated_data')\n",
    "#processed_stock_list = os.listdir('G:/My Drive/UdS/Classes/Data Science/truncated_data')\n",
    "\n",
    "columns = [\"Low\",\"Open\",\"Volume\",\"High\",\"Close\",\"Adjusted Close\",\"Annual Percent Change\",\"positive\",\"negative\",\"neutral\",\"Label\"]\n",
    "full_dataset = pd.DataFrame(columns=columns)\n",
    "for stock in tqdm(truncated_stock_list):\n",
    "    if stock == 'desktop.ini':\n",
    "        continue\n",
    "    stock_data = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/stock_data/' + stock)\n",
    "    try:\n",
    "        full_dataset = full_dataset.append(stock_data, ignore_index=True)\n",
    "    except IndexError:\n",
    "        print(stock)\n",
    "        print(stock_data)\n",
    "        print(stock_data.head())\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = full_dataset[\"Label\"]\n",
    "X = full_dataset.drop(columns=[\"Label\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 80:20 train to test\n",
    "X_test, X_dev, y_test, y_dev = train_test_split(X_test, y_test, test_size=0.5, random_state=1) # 50:50 test to dev > 80:10:10 train:dev:test\n",
    "\n",
    "# write splits to individual csv files\n",
    "# X_train.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_train.csv', index=False)\n",
    "# X_dev.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_dev.csv', index=False)\n",
    "# X_test.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_test.csv', index=False)\n",
    "# y_train.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_train.csv', index=False)\n",
    "# y_dev.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_dev.csv', index=False)\n",
    "# y_test.to_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45859, 10)\n",
      "(5733, 10)\n",
      "(5732, 10)\n",
      "(45859, 1)\n",
      "(5733, 1)\n",
      "(5732, 1)\n"
     ]
    }
   ],
   "source": [
    "# reload splits from csv files\n",
    "X_train = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_train.csv')\n",
    "X_dev = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_dev.csv')\n",
    "X_test = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/X_test.csv')\n",
    "y_train = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_train.csv')\n",
    "y_dev = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_dev.csv')\n",
    "y_test = pd.read_csv('G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/finalized_data/y_test.csv')\n",
    "\n",
    "# verify data integrity\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_dev.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (7): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_X = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "train_y = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "traindata = MyDataset(train_X, train_y)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(10, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 1),\n",
    "        nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    for X, y in trainloader:\n",
    "        X = X\n",
    "        y = y.squeeze(1)\n",
    "        y_pred = model(X).squeeze(1)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(model)\n",
    "\n",
    "torch.save(model, \"G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/models/multilayer_model.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (7): Sigmoid()\n",
      ")\n",
      "Predicted     0   All\n",
      "True                 \n",
      "0          4970  4970\n",
      "1           763   763\n",
      "All        5733  5733\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m confusion \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mcrosstab(dev_predictions[\u001b[39m'\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m'\u001b[39m], dev_predictions[\u001b[39m'\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m'\u001b[39m], rownames\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     38\u001b[0m                         colnames\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mPredicted\u001b[39m\u001b[39m'\u001b[39m], margins\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(confusion)\n\u001b[1;32m---> 41\u001b[0m accuracy \u001b[39m=\u001b[39m (confusion[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m confusion[\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m confusion[\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(accuracy)\n\u001b[0;32m     43\u001b[0m precision \u001b[39m=\u001b[39m confusion[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m confusion[\u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3458\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3459\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3460\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n\u001b[0;32m   3366\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "dev_X = torch.tensor(X_dev.values, dtype=torch.float32)\n",
    "dev_y = torch.tensor(y_dev.values, dtype=torch.long)\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "devdata = MyDataset(dev_X, dev_y)\n",
    "devloader = torch.utils.data.DataLoader(devdata, batch_size=64, shuffle=True)\n",
    "\n",
    "model = torch.load(\"G:/My Drive/UdS/Classes/Data Science/DS-Miniproject/models/multilayer_model.pickle\")\n",
    "print(model)\n",
    "model.eval()\n",
    "\n",
    "dev_predictions = pd.DataFrame(columns=['y_pred', 'y_true'])\n",
    "with torch.no_grad():\n",
    "    for X, y in devloader:\n",
    "        X = X\n",
    "        y = y.squeeze(1).tolist()\n",
    "        y_pred = [int(y_val) for y_val in model(X).squeeze(1).tolist()]\n",
    "        y_frame = pd.DataFrame({'y_pred': y_pred, 'y_true': y})\n",
    "        dev_predictions = dev_predictions.append(y_frame, ignore_index=True)\n",
    "\n",
    "confusion = pd.crosstab(dev_predictions['y_true'], dev_predictions['y_pred'], rownames=['True'], \n",
    "                        colnames=['Predicted'], margins=True)\n",
    "print(confusion)\n",
    "\n",
    "accuracy = (confusion[0][0] + confusion[1][1]) / confusion['All']['All']\n",
    "print(accuracy)\n",
    "precision = confusion[1][1] / confusion['All'][1]\n",
    "print(precision)\n",
    "recall = confusion[1][1] / confusion[1]['All']\n",
    "print(recall)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
